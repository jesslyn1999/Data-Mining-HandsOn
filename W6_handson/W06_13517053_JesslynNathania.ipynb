{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 06 Handson - Text Classifier\n",
    "In this week hands-on, we will create a sentiment analyzer on twitter using the concept of classification and text pre-processing that we have learned before. We will cover:<br>\n",
    "a. text pre-processing,<br>\n",
    "b. splitting data for training & testing and converting them into numerical features,<br>\n",
    "c. training a classifier model and perform predictions on testing dataset,<br>\n",
    "d. Evaluating performance of algorithm<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset \"tweets.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1038</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>that film is fantastic #brilliant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1804</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>this music is really bad #myband</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1693</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>winter is terrible #thumbs-down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1477</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>this game is awful #nightmare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I love jam #loveit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource                      SentimentText\n",
       "0    1038          1    Sentiment140  that film is fantastic #brilliant\n",
       "1    1804          1    Sentiment140   this music is really bad #myband\n",
       "2    1693          0    Sentiment140    winter is terrible #thumbs-down\n",
       "3    1477          0    Sentiment140      this game is awful #nightmare\n",
       "4      45          1    Sentiment140                 I love jam #loveit"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('./tweets.csv', sep=\",\")# adjust with your own path\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 01 (W01)\n",
    "The given dataset is still a 'raw dataset' which includes some unwanted features, unwanted characters, etc.<br>\n",
    "a. Select the `SentimentText` column as an attribute and the `Sentiment` column as a label (ground truth) for this study case<br><br>\n",
    "b. In this M01.b, you have been provided a function template `pre_process` (see below) to perform all the pre-processing step to the all tweet data in the dataset. Complete pre-process function with all techniques that you have learned in the previous hands-on week (W03) for text pre-processing, so the all text attributes can be converted to `pre-processed text`, e.g., after being applied by: (i) tokenization, (ii) normalization, (iii) cleaning, (iv) stemming or lemmatization. Here, you will get `list of words`.<br><br>\n",
    "c. Use the function that you have completed in M01.b, looped for each data row of `SentimentText` column. For each looping, you will get `list of words`. Append this `list of words` for each looping result in to list, so, will get `list of list`.<br><br>\n",
    "\n",
    "d. Split (random & stratified) `list of list` you get in M01.c into `training data` and `testing data`. The testing dataset must be 20% from overall dataset. Print the total number of initial dataset, total number of training dataset and testing dataset. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that film is fantastic #brilliant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this music is really bad #myband</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>winter is terrible #thumbs-down</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this game is awful #nightmare</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love jam #loveit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I dislike skiing #rubbish</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I like pop music #toptastic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this game is awful good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rock music is terrible #worstever</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that movie is great #favorite</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           attribute label\n",
       "0  that film is fantastic #brilliant     1\n",
       "1   this music is really bad #myband     1\n",
       "2    winter is terrible #thumbs-down     0\n",
       "3      this game is awful #nightmare     0\n",
       "4                 I love jam #loveit     1\n",
       "5          I dislike skiing #rubbish     0\n",
       "6        I like pop music #toptastic     1\n",
       "7            this game is awful good     1\n",
       "8  rock music is terrible #worstever     0\n",
       "9      that movie is great #favorite     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put your code here for M01.a\n",
    "df = pd.DataFrame(np.array([tweets[\"SentimentText\"], tweets[\"Sentiment\"]]).T, columns=[\"attribute\", \"label\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'pop', 'music', 'toptastic']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put your code here for M01.b\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatize = WordNetLemmatizer().lemmatize\n",
    "\n",
    "\n",
    "def pre_process(input_ori):\n",
    "    '''\n",
    "    Write code implementation for text pre-processing here. \n",
    "    Use what you have learned before about text pre-processing.\n",
    "    \n",
    "    Parameter:\n",
    "    input_ori = raw data text (single tweet data)\n",
    "    \n",
    "    Return value:\n",
    "    processed_tweet = 'list of words'\n",
    "    '''\n",
    "    # (i)tokenization: word_tokenize, (ii)normalization:lower_case\n",
    "    tokens = [token.lower() for token in word_tokenize(input_ori)]\n",
    "    # (iii)cleaning: remove punctuation, filter isalpha and not stopwords, (iv)lemmatization\n",
    "    processed_tweet = [lemmatize(token.translate(table)) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    \n",
    "    return processed_tweet\n",
    "\n",
    "\n",
    "pre_process(df[\"attribute\"][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'fantastic', 'brilliant']: 1\n",
      "['music', 'really', 'bad', 'myband']: 1\n",
      "['winter', 'terrible']: 0\n",
      "['game', 'awful', 'nightmare']: 0\n",
      "['love', 'jam', 'loveit']: 1\n",
      "['dislike', 'skiing', 'rubbish']: 0\n",
      "['like', 'pop', 'music', 'toptastic']: 1\n",
      "['game', 'awful', 'good']: 1\n",
      "['rock', 'music', 'terrible', 'worstever']: 0\n",
      "['movie', 'great', 'favorite']: 1\n"
     ]
    }
   ],
   "source": [
    "#put your code here for M01.c\n",
    "list_of_list = df[\"attribute\"].apply(pre_process)\n",
    "list_of_label =df[\"label\"]\n",
    "\n",
    "for x, y in zip(list_of_list, list_of_label[:10]):\n",
    "    print(\"{}: {}\".format(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of init data: 1932, training_data=1545, test_data=387\n"
     ]
    }
   ],
   "source": [
    "#put your code here for M01.d\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# sss.get_n_splits(list_of_list, list_of_label)\n",
    "train_idx, test_idx = next(sss.split(list_of_list, list_of_label))\n",
    "\n",
    "training_data, training_label = list_of_list[train_idx], list(list_of_label[train_idx])\n",
    "test_data, test_label = list_of_list[test_idx], list(list_of_label[test_idx])\n",
    "\n",
    "print(\"length of init data: {}, training_data={}, test_data={}\".format(\n",
    "len(list_of_list), len(training_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M02\n",
    "a. Build `tfidf_model` by using codes below with `training data` you get in M01.d. (`TfidfVectorizer` is from scikit-learn)\n",
    "```\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None)\n",
    "```\n",
    "b. Transform `training data` and `testing data` you get in M01.d by using `tfidf_model` you get in M02.a. In this case, you will get numerical features, both from `training data` and `testing data`.<br><br>\n",
    "c. Choose a classification algorithm (you may use library such as scikit-learn), and explain why you choose it.<br><br>\n",
    "d. Train the classifier model, based on the algorithm you have chosen, by using numerical features of `training data` from M02.b.<br><br>\n",
    "e. Make predictions on the numerical features of `testing dataset` you get in M02.b using the classifier model that you have trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf model shape=44\n",
      "['adore', 'awesome', 'awful', 'bad', 'band', 'best', 'bestever', 'book', 'brilliant', 'cheese'], ...\n"
     ]
    }
   ],
   "source": [
    "# put your code here for M02.a\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None)\n",
    "\n",
    "tfidf_model = tfidf.fit(training_data)\n",
    "tfidf_feats = tfidf_model.get_feature_names()\n",
    "print(\"tfidf model shape={}\\n{}, ...\".format(len(tfidf_feats), tfidf_feats[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc term matrix shape train=(1545, 44), test=(387, 44)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.47193179, 0.        ,\n",
       "       0.        , 0.66637412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.57725723, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put your code here for M02.b\n",
    "train_numfeats = tfidf_model.transform(training_data).toarray()\n",
    "test_numfeats = tfidf_model.transform(test_data).toarray()\n",
    "print(\"doc term matrix shape train={}, test={}\".format(train_numfeats.shape, test_numfeats.shape))\n",
    "train_numfeats[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your explanation (M02.c):\n",
    "\n",
    "**eksperimen menggunakan SVM(support vector machine) supervised model dengan estimator SVC(support vector classification).** SVM merupakan model yang paling sering digunakan untuk analisi sentimen. Beberapa riset juga telah membuktikan bahwa SVM cocok digunakan untuk interpretasi sosial, paling cocok untuk analisis sentimen. SVM cocok untuk klasifikasi teks dikarenakan kemampuannya untuk mengatasi fitur atau dimensi yang berjumlah banyak. Kelebihan lainnya meliputi bahwa model SVM kuat ketika menghadapi data yang sangat jarang ditemui dikarenakan kebanyakan masalah data ini ialah *linearly separable*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       779\n",
      "           1       0.98      0.99      0.98       766\n",
      "\n",
      "    accuracy                           0.98      1545\n",
      "   macro avg       0.98      0.98      0.98      1545\n",
      "weighted avg       0.98      0.98      0.98      1545\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      " [[762  17]\n",
      " [  7 759]]\n"
     ]
    }
   ],
   "source": [
    "# put your code here for M02.d\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "svc_model = SVC(C=1.0, kernel='rbf', degree=3, gamma='scale').fit(train_numfeats, training_label)\n",
    "all_predictions = svc_model.predict(train_numfeats)\n",
    "\n",
    "print(\"Classification report :\\n\", classification_report(training_label, all_predictions))\n",
    "print(\"\\n\\nConfusion matrix: \\n\", confusion_matrix(training_label, all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here for M02.e\n",
    "test_predictions = svc_model.predict(test_numfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M03\n",
    "After train the classification model and make prediction using that model, now you will evaluate the performance of your model against testing dataset.<br>\n",
    "a. Calculate and print the accuracy of your model's predictions in M02.e against testing dataset ground truth<br>\n",
    "b. What you can infer based on the result?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       195\n",
      "           1       0.99      0.99      0.99       192\n",
      "\n",
      "    accuracy                           0.99       387\n",
      "   macro avg       0.99      0.99      0.99       387\n",
      "weighted avg       0.99      0.99      0.99       387\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      " [[194   1]\n",
      " [  1 191]]\n"
     ]
    }
   ],
   "source": [
    "#put your code here for Q03.a\n",
    "\n",
    "print(\"Classification report :\\n\", classification_report(test_label, test_predictions))\n",
    "print(\"\\n\\nConfusion matrix: \\n\", confusion_matrix(test_label, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer (M03.b) :\n",
    "\n",
    "Akurasi dalam eksperimen ini yaitu 0.99. Akurasi dari model SVM ini mungkin dapat ditingkatkan lagi dengan mengubah hyperparameter sendiri melalui validation_testing dan juga melalui proses POS_tagger dahulu sebelum menjalani training model. Hasil akurasi ini tinggi sesuai dengan eksperimen riset, yaitu SVM cocok digunakan untuk analisis sentimen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_venv",
   "language": "python",
   "name": "global_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
