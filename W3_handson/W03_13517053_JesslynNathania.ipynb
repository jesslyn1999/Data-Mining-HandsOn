{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week  03 Handson - Data Preprocessing #02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens1:\n",
      " ['After', 'watching', 'two', 'hours', 'non', 'stop', ',', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', '#', 'brilliant', '.']\n",
      "\n",
      "tokens2:\n",
      " ['Foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', ',', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', '#', 'notrecommended', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text1 = \"After watching two hours non stop, he says that \\\n",
    "        the film is really fantastic #brilliant.\"\n",
    "text2 = \"Foods sold there are little bit pricy, \\\n",
    "        meanwhile the taste is not delicious #notrecommended.\"\n",
    "\n",
    "tokens1 = word_tokenize(text1)\n",
    "print(\"tokens1:\\n\", tokens1)\n",
    "\n",
    "tokens2 = word_tokenize(text2)\n",
    "print(\"\\ntokens2:\\n\", tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_words1:\n",
      " ['after', 'watching', 'two', 'hours', 'non', 'stop', ',', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', '#', 'brilliant', '.']\n",
      "\n",
      "\n",
      "normalized_words2:\n",
      " ['foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', ',', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', '#', 'notrecommended', '.']\n"
     ]
    }
   ],
   "source": [
    "normalized_words1 = [w.lower() for w in tokens1]\n",
    "print(\"normalized_words1:\\n\", normalized_words1)\n",
    "\n",
    "normalized_words2 = [w.lower() for w in tokens2]\n",
    "print(\"\\n\\nnormalized_words2:\\n\", normalized_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning 01: remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punc_removed1:\n",
      " ['after', 'watching', 'two', 'hours', 'non', 'stop', '', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', '', 'brilliant', '']\n",
      "\n",
      "\n",
      "punc_removed2:\n",
      " ['foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', '', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', '', 'notrecommended', '']\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "punc_removed1 = [w.translate(table) for w in normalized_words1]\n",
    "print(\"punc_removed1:\\n\", punc_removed1)\n",
    "\n",
    "punc_removed2 = [w.translate(table) for w in normalized_words2]\n",
    "print(\"\\n\\npunc_removed2:\\n\", punc_removed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning 02: remove not alphabetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isalpha_words1:\n",
      " ['after', 'watching', 'two', 'hours', 'non', 'stop', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', 'brilliant']\n",
      "\n",
      "\n",
      "isalpha_words2:\n",
      " ['foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', 'notrecommended']\n"
     ]
    }
   ],
   "source": [
    "isalpha_words1 = [word for word in punc_removed1 if word.isalpha()]\n",
    "print(\"isalpha_words1:\\n\", isalpha_words1)\n",
    "\n",
    "isalpha_words2 = [word for word in punc_removed2 if word.isalpha()]\n",
    "print(\"\\n\\nisalpha_words2:\\n\", isalpha_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning 03: remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopWords_removed1:\n",
      " ['watching', 'two', 'hours', 'non', 'stop', 'says', 'film', 'really', 'fantastic', 'brilliant']\n",
      "\n",
      "\n",
      "stopWords_removed2:\n",
      " ['foods', 'sold', 'little', 'bit', 'pricy', 'meanwhile', 'taste', 'delicious', 'notrecommended']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# print(\"stop_words:\\n{}\\n\".format(stop_words))\n",
    "\n",
    "stopWords_removed1 = [w for w in isalpha_words1 if not w in stop_words]\n",
    "print(\"stopWords_removed1:\\n\", stopWords_removed1)\n",
    "\n",
    "stopWords_removed2 = [w for w in isalpha_words2 if not w in stop_words]\n",
    "print(\"\\n\\nstopWords_removed2:\\n\", stopWords_removed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed_word1: \n",
      " ['watch', 'two', 'hour', 'non', 'stop', 'say', 'film', 'realli', 'fantast', 'brilliant']\n",
      "\n",
      "\n",
      "stemmed_word2:\n",
      " ['food', 'sold', 'littl', 'bit', 'prici', 'meanwhil', 'tast', 'delici', 'notrecommend']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_word1 = [ps.stem(w) for w in stopWords_removed1]\n",
    "print(\"stemmed_word1: \\n\", stemmed_word1)\n",
    "\n",
    "stemmed_word2 = [ps.stem(w) for w in stopWords_removed2]\n",
    "print(\"\\n\\nstemmed_word2:\\n\", stemmed_word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_words1:\n",
      " ['watching', 'two', 'hour', 'non', 'stop', 'say', 'film', 'really', 'fantastic', 'brilliant']\n",
      "\n",
      "\n",
      "lemmatized_words2:\n",
      " ['food', 'sold', 'little', 'bit', 'pricy', 'meanwhile', 'taste', 'delicious', 'notrecommended']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words1 = [lemmatizer.lemmatize(w) for w in stopWords_removed1]\n",
    "print(\"lemmatized_words1:\\n\", lemmatized_words1)\n",
    "\n",
    "lemmatized_words2 = [lemmatizer.lemmatize(w) for w in stopWords_removed2]\n",
    "print(\"\\n\\nlemmatized_words2:\\n\", lemmatized_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Converting Preprocessed Text into Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names:\n",
      " ['bit', 'brilliant', 'delicious', 'fantastic', 'film', 'food', 'hour', 'little', 'meanwhile', 'non', 'notrecommended', 'pricy', 'really', 'say', 'sold', 'stop', 'taste', 'two', 'watching']\n",
      "\n",
      "\n",
      "numerical_features of text1: \n",
      " [0.         0.31622777 0.         0.31622777 0.31622777 0.\n",
      " 0.31622777 0.         0.         0.31622777 0.         0.\n",
      " 0.31622777 0.31622777 0.         0.31622777 0.         0.31622777\n",
      " 0.31622777] ; shape:  (19,)\n",
      "\n",
      "\n",
      "numerical_features of text2:\n",
      " [0.33333333 0.         0.33333333 0.         0.         0.33333333\n",
      " 0.         0.33333333 0.33333333 0.         0.33333333 0.33333333\n",
      " 0.         0.         0.33333333 0.         0.33333333 0.\n",
      " 0.        ] ; shape:  (19,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "\n",
    "two_preprocessed_text = [lemmatized_words1, lemmatized_words2]\n",
    "\n",
    "# define the tfidf vectorizer\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=dummy,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None)\n",
    "\n",
    "# train / Learn from the given data\n",
    "model = tfidf.fit(two_preprocessed_text)\n",
    "\n",
    "# transform to numerical features using the trained model\n",
    "numerical_features = model.transform(two_preprocessed_text).toarray()\n",
    "\n",
    "print(\"feature names:\\n\", tfidf.get_feature_names())\n",
    "print(\"\\n\\nnumerical_features of text1: \\n\", numerical_features[0],\n",
    "     \"; shape: \", numerical_features[0].shape)\n",
    "print(\"\\n\\nnumerical_features of text2:\\n\", numerical_features[1],\n",
    "     \"; shape: \", numerical_features[1].shape)\n",
    "\n",
    "# list(filter(lambda x: x>0, numerical_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. ***Tokenization*** adalah process  melakukan *split* terhadap *string* yang diberikan. Salah satu bagian teknik *tokenization* yang ada, yaitu *word_tokenize*, yaitu *split* dilakukan dengan tanda baca sebagai pemisah; *wordpuct_tokenize* juga melakukan hal yang sama, namun bedanya *word_tokenize* mampu melakukan split kalimat $3.88 menjadi `[ '$', '3.88']`, sedangkan *wordpuct_tokenize* memberikan jawaban yang beda yaitu, `['$', '3', '.', '88']`. Selain itu juga terdapat *sent_tokenize*, yaitu melakukan split string menjadi per kalimat. \n",
    "\n",
    "***Normalization*** yaitu proses mengubah teks dalam format yang sama. Pada contoh di atas *normalization* dilakukan dengan cara mengubah *string* menjadi *lowercase*.\n",
    "\n",
    "***Cleaning*** yaitu proses yang menghilangkan teks-teks yang tidak bermakna, seperti *stopwords*, tanda baca, dan *empty string*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Keduanya memiliki tujuan yang sama yaitu mengambil format kata yang umum atau dasar dari kata yang diberikan dapat berupa kata berimbuhan.\n",
    "\n",
    "***Stemming*** mengambil kata dasar dari suatu kata yang diberikan. Sudah terdapat *dictionaries* yang menampung daftar kata berserta dengan *prefix* dan *suffix*-nya masing-masing. Sehingga *output* dari teknik ini yaitu kata yang telah dikurangi *prefix* dan *suffix*.\n",
    "\n",
    "***Lemmatization*** di lain sisi, memiliki tujuan lebih ke *linguistics* dan tidak langsung mengambil kata dasar, melainkan *dictionaries* dan algoritmanya lebih kompleks dan lebih mengerti tentang morfologi struktur kalimat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TF-IDF***(*term-frequency times inverse document-frequency*) menunjukkan seberapa penting suatu kata/istilah/*term* di dalam dokumen untuk diperoleh informasi bersangkutan, biasanya cocok digunakan untuk klasifikasi dokumen. ***TF-IDF*** memberikan nilai pada suatu istilah di dalam dokumen berdasarkan proporsi inverse dari frekuensi istilah di dokumen dibandingkan keseluruhan persen dokumen dimana istilah itu muncul. Semua istilah dalam suatu dokumen tidak boleh diperlakukan secara sama berdasarkan aspek frekuensi saja. *Zipf's* law mengatakan, kata fungsional seperti *stopwords* biasanya bersifat dominan distribusinya dalam dokumen.\n",
    "\n",
    "$tf-idf(t, d) = tf(t, d) * idf(t)$, dimana t adalah *term* dari suatu *document* d adalah di dalam suatu set dokumen.\n",
    "\n",
    "$tf(t, d)$ adalah *term frequency* adalah jumlah frekuensi *term* t muncul di *document* d. Formula ini tidak cocok digunakan secara mentah karena jika dalam suatu dokumen, kemunculan suatu istilah tertentu ialah 10 kali namun tidak berarti ia lebih relevan 10 kali dibandingkan dokumen lain dengan kemunculan suatu istilah tersebut satu kali saja. Oleh karena itu, kemudian muncul formula $tf-idf(t, d)$.\n",
    "\n",
    "$idf(t) = $log(n / df(t))$ + 1$, jika smooth_idf=False, dimana n adalah total jumlah *documents* atau kalimat di dalam set dokumen dan $df(t)$ adalah frekuensi dokumen dari t. Formula di sklearn ini berbeda dari yang ditetapkan secara standar.\n",
    "\n",
    "<br>\n",
    "\n",
    "Istilah dengan nilai ***TF-IDF*** yang tinggi menunjukkan ikatan yang kuat dengan dokumen dimana istilah ini muncul, dengan kata lain, jika kata tersebut muncul dalam sebuah *query*, dokumen itu menarik bagi *users*. *Array* eksperimen di atas mempunyai *shape* 19 dikarenakan total istilah unik di dalam dua dokumen tersebut berjumlah 19.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Alternative to TF-IDF Vectorizer***: \n",
    "\n",
    "1. *Hashing Vectorizer*\n",
    "<br>\n",
    "Mengubah teks dokumen menjadi matrix yang mengandung jumlah kemunculan token atau informasi kemunculan binary dengan mengaplikasikan trik hashing untuk mencari token string sebagai karakteristik index mapping.\n",
    "\n",
    "\n",
    "2. *Count Vectorizer*\n",
    "<br>\n",
    "Mengubah teks ke matriks mengandung nilai jumlah token. Output berupa nilai 0 atau 1. 1 menunjukkan bermakna dan 0 sebaliknya."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_venv",
   "language": "python",
   "name": "global_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
